{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROYECTO FINAL - APACHE SPARK\n",
    "\n",
    "## Caso:\n",
    "\n",
    "La pizzeria Danny quiere usar los datos para responder algunas preguntas simples sobre sus\n",
    "clientes, especialmente sobre sus patrones de visitas, cuánto dinero han gastado y también qué\n",
    "elementos del menú son sus favoritos.\n",
    "\n",
    "![](diagram.png)\n",
    "\n",
    "* Preguntas a resolver usando Apache Spark:\n",
    "    \n",
    "    1. ¿Cuál es la cantidad total que gastó cada cliente en el restaurante?\n",
    "\n",
    "    2. ¿Cuántos días ha visitado cada cliente el restaurante?\n",
    "    \n",
    "    3. ¿Cuál fue el primer artículo del menú comprado por cada cliente?\n",
    "    \n",
    "    4. ¿Cuál es el artículo más comprado en el menú y cuántas veces lo compraron todos los\n",
    "    clientes?\n",
    "\n",
    "    5. ¿Qué artículo fue el más popular para cada cliente?\n",
    "\n",
    "    6. ¿Qué artículo compró primero el cliente después de convertirse en miembro?\n",
    "\n",
    "    7. ¿Qué artículo se compró justo antes de que el cliente se convirtiera en miembro?\n",
    "    \n",
    "    8. ¿Cuál es el total de artículos y la cantidad gastada por cada miembro antes de\n",
    "    convertirse en miembro?\n",
    "    \n",
    "    9. Si cada $ 1 gastado equivale a 10 puntos y el sushi tiene un multiplicador de puntos 2x, ¿cuántos puntos tendría cada cliente?\n",
    "    \n",
    "    10. En la primera semana después de que un cliente se une al programa (incluida la fecha\n",
    "    de ingreso), gana el doble de puntos en todos los artículos, no solo en sushi. ¿Cuántos\n",
    "    puntos tienen los clientes A y B a fines de enero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.8:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[3]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f35a0075bb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "spark = SparkSession.builder.\\\n",
    "                    master(\"local[3]\").\\\n",
    "                    config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.23.1').\\\n",
    "                    getOrCreate()\n",
    "                    \n",
    "                    \n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|customer_id| join_date|\n",
      "+-----------+----------+\n",
      "|          A|2021-01-07|\n",
      "|          B|2021-01-09|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url_members = \"https://raw.githubusercontent.com/datahackformation/DEP7_Apache-Spark/main/Trabajo%20Final/Data/members.csv\"\n",
    "spark.sparkContext.addFile(url_members)\n",
    "members = spark.read.option(\"header\", True).csv(SparkFiles.get(\"members.csv\"),inferSchema=True)\n",
    "members.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- join_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "members.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----+\n",
      "|product_id|product_name|price|\n",
      "+----------+------------+-----+\n",
      "|         1|       sushi|   10|\n",
      "|         2|       curry|   15|\n",
      "|         3|       ramen|   12|\n",
      "+----------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url_menu = \"https://raw.githubusercontent.com/datahackformation/DEP7_Apache-Spark/main/Trabajo%20Final/Data/menu.csv\"\n",
    "spark.sparkContext.addFile(url_menu)\n",
    "menu = spark.read.option(\"header\", True).csv(SparkFiles.get(\"menu.csv\"),inferSchema=True)\n",
    "menu.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "menu.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+\n",
      "|customer_id|order_date|product_id|\n",
      "+-----------+----------+----------+\n",
      "|          A|2021-01-01|         1|\n",
      "|          A|2021-01-01|         2|\n",
      "|          A|2021-01-07|         2|\n",
      "|          A|2021-01-10|         3|\n",
      "|          A|2021-01-11|         3|\n",
      "|          A|2021-01-11|         3|\n",
      "|          B|2021-01-01|         2|\n",
      "|          B|2021-01-02|         2|\n",
      "|          B|2021-01-04|         1|\n",
      "|          B|2021-01-11|         1|\n",
      "|          B|2021-01-16|         3|\n",
      "|          B|2021-02-01|         3|\n",
      "|          C|2021-01-01|         3|\n",
      "|          C|2021-01-01|         3|\n",
      "|          C|2021-01-07|         3|\n",
      "+-----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url_sales = \"https://raw.githubusercontent.com/datahackformation/DEP7_Apache-Spark/main/Trabajo%20Final/Data/sales.csv\"\n",
    "spark.sparkContext.addFile(url_sales)\n",
    "sales = spark.read.option(\"header\", True).csv(SparkFiles.get(\"sales.csv\"),inferSchema=True)\n",
    "sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 1: \n",
    "\n",
    "¿Cuál es la cantidad total que gastó cada cliente en el restaurante?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|customer_id|sum(price)|\n",
      "+-----------+----------+\n",
      "|A          |76        |\n",
      "|B          |74        |\n",
      "|C          |36        |\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.join(menu,sales.product_id ==  menu.product_id,\"inner\") \\\n",
    "     .select('customer_id','price')\\\n",
    "     .groupBy('customer_id').sum('price')\\\n",
    "     .orderBy('customer_id')\\\n",
    "     .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 2:\n",
    " \n",
    "¿Cuántos días ha visitado cada cliente el restaurante?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|customer_id|count(order_date)|\n",
      "+-----------+-----------------+\n",
      "|A          |4                |\n",
      "|B          |6                |\n",
      "|C          |2                |\n",
      "+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "sales.select('customer_id','order_date')\\\n",
    "     .groupBy('customer_id').agg(countDistinct('order_date'))\\\n",
    "     .orderBy('customer_id')\\\n",
    "     .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 3:\n",
    " \n",
    "¿Cuál fue el primer artículo del menú comprado por cada cliente?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------------+\n",
      "|customer_id|order_date|product_name|\n",
      "+-----------+----------+------------+\n",
      "|          A|2021-01-01|       sushi|\n",
      "|          B|2021-01-01|       curry|\n",
      "|          C|2021-01-01|       ramen|\n",
      "+-----------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Usando funciones spark\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number\n",
    "\n",
    "w2 = Window.partitionBy(\"customer_id\").orderBy(col('order_date'))\n",
    "sales.withColumn(\"row\",row_number().over(w2))\\\n",
    "  .filter(col(\"row\") == 1).drop(\"row\")\\\n",
    "  .join(menu,sales.product_id ==  menu.product_id,\"inner\")\\\n",
    "  .select('customer_id','order_date','product_name')\\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------------+\n",
      "|customer_id|order_date|product_name|\n",
      "+-----------+----------+------------+\n",
      "|          A|2021-01-01|       sushi|\n",
      "|          B|2021-01-01|       curry|\n",
      "|          C|2021-01-01|       ramen|\n",
      "+-----------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Usando SQL query\n",
    "sales.createOrReplaceTempView(\"Sales\")\n",
    "menu.createOrReplaceTempView(\"Menu\")\n",
    "spark.sql(\"select customer_id, order_date, product_name from \"+\n",
    "     \" (select *, row_number() OVER (PARTITION BY customer_id ORDER BY order_date) as rn \" +\n",
    "     \" FROM Sales) tmp inner join Menu as M on tmp.product_id = M.product_id where rn = 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 4:\n",
    " \n",
    "¿Cuál es el artículo más comprado en el menú y cuántas veces lo compraron todos los clientes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----+\n",
      "|customer_id|product_name|count|\n",
      "+-----------+------------+-----+\n",
      "|          A|       ramen|    3|\n",
      "|          B|       sushi|    2|\n",
      "|          B|       ramen|    2|\n",
      "|          B|       curry|    2|\n",
      "|          C|       ramen|    3|\n",
      "+-----------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Usando funciones Spark\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "w = Window.partitionBy('customer_id')\n",
    "sales.groupBy('customer_id','product_id').count()\\\n",
    "     .withColumn('maxCount', f.max('count').over(w))\\\n",
    "     .where(f.col('count') == f.col('maxCount')).drop('maxCount')\\\n",
    "     .join(menu,sales.product_id == menu.product_id)\\\n",
    "     .select('customer_id','product_name','count')\\\n",
    "     .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----+\n",
      "|customer_id|product_name|count|\n",
      "+-----------+------------+-----+\n",
      "|          A|       ramen|    3|\n",
      "|          B|       sushi|    2|\n",
      "|          B|       ramen|    2|\n",
      "|          B|       curry|    2|\n",
      "|          C|       ramen|    3|\n",
      "+-----------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.createOrReplaceTempView(\"Sales\")\n",
    "menu.createOrReplaceTempView(\"Menu\")\n",
    "spark.sql(\"select customer_id, product_name, count from \" +\n",
    "          \"(select *, MAX(count) OVER (PARTITION BY customer_id) AS maxCount from \" +\n",
    "          \"(select customer_id, product_id, count(product_id) as count from Sales group by customer_id, product_id) tmp) tf \" +\n",
    "          \"inner join Menu as M on tf.product_id = M.product_id where maxCount = count\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 5:\n",
    " \n",
    "¿Qué artículo fue el más popular para cada cliente?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|customer_id|product_name|\n",
      "+-----------+------------+\n",
      "|          A|       ramen|\n",
      "|          B|       sushi|\n",
      "|          B|       ramen|\n",
      "|          B|       curry|\n",
      "|          C|       ramen|\n",
      "+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "w = Window.partitionBy('customer_id')\n",
    "sales.groupBy('customer_id','product_id').count()\\\n",
    "     .withColumn('maxCount', f.max('count').over(w))\\\n",
    "     .where(f.col('count') == f.col('maxCount')).drop('maxCount')\\\n",
    "     .join(menu,sales.product_id == menu.product_id)\\\n",
    "     .select('customer_id','product_name')\\\n",
    "     .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|customer_id|product_name|\n",
      "+-----------+------------+\n",
      "|          A|       ramen|\n",
      "|          B|       sushi|\n",
      "|          B|       ramen|\n",
      "|          B|       curry|\n",
      "|          C|       ramen|\n",
      "+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.createOrReplaceTempView(\"Sales\")\n",
    "menu.createOrReplaceTempView(\"Menu\")\n",
    "spark.sql(\"select customer_id, product_name from\" +\n",
    "          \"(select customer_id, product_id, count, MAX(count) OVER (PARTITION BY customer_id) AS maxCount from\" +\n",
    "          \"(select customer_id, product_id, count(product_id) as count from Sales group by customer_id, product_id) tmp) tf \" +\n",
    "          \"inner join Menu as m on tf.product_id = m.product_id where maxCount = count\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 6:\n",
    " \n",
    "¿Qué artículo compró primero el cliente después de convertirse en miembro?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+----------+\n",
      "|customer_id|order_date|product_id|join_date |\n",
      "+-----------+----------+----------+----------+\n",
      "|A          |2021-01-07|2         |2021-01-07|\n",
      "|B          |2021-01-11|1         |2021-01-09|\n",
      "+-----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w6 = Window.partitionBy(\"customer_id\").orderBy(col('order_date'))\n",
    "sales.join(members, [\"customer_id\"],\"inner\")\\\n",
    "     .where(col(\"order_date\")>=col(\"join_date\"))\\\n",
    "     .withColumn(\"row\",row_number().over(w6))\\\n",
    "     .filter(col(\"row\")==1).drop(\"row\")\\\n",
    "     .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+----------+\n",
      "|customer_id|order_date|product_id| join_date|\n",
      "+-----------+----------+----------+----------+\n",
      "|          A|2021-01-07|         2|2021-01-07|\n",
      "|          B|2021-01-11|         1|2021-01-09|\n",
      "+-----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.createOrReplaceTempView(\"Sales\")\n",
    "members.createOrReplaceTempView(\"Members\")\n",
    "spark.sql(\"select customer_id, order_date, product_id, join_date \"+\n",
    "          \"from (select *, row_number() OVER (PARTITION BY customer_id ORDER BY order_date) as rn from \"+\n",
    "          \"(select s.customer_id as customer_id, order_date, product_id, join_date from Sales s \"+ \n",
    "          \"inner join Members m on s.customer_id = m.customer_id where order_date >= join_date) temp) tf \"+\n",
    "          \"where rn = 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 7:\n",
    "\n",
    "¿Qué artículo se compró justo antes de que el cliente se convirtiera en miembro?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+----------+\n",
      "|customer_id|order_date|product_id|join_date |\n",
      "+-----------+----------+----------+----------+\n",
      "|A          |2021-01-01|1         |2021-01-07|\n",
      "|B          |2021-01-04|1         |2021-01-09|\n",
      "+-----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w6 = Window.partitionBy(\"customer_id\").orderBy(col('order_date').desc())\n",
    "sales.join(members, [\"customer_id\"],\"inner\")\\\n",
    "     .where(col(\"order_date\")<col(\"join_date\"))\\\n",
    "     .withColumn(\"row\",row_number().over(w6))\\\n",
    "     .filter(col(\"row\")==1).drop(\"row\")\\\n",
    "     .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+----------+\n",
      "|customer_id|order_date|product_id| join_date|\n",
      "+-----------+----------+----------+----------+\n",
      "|          A|2021-01-01|         1|2021-01-07|\n",
      "|          B|2021-01-04|         1|2021-01-09|\n",
      "+-----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.createOrReplaceTempView(\"Sales\")\n",
    "members.createOrReplaceTempView(\"Members\")\n",
    "spark.sql(\"select customer_id, order_date, product_id, join_date \"+\n",
    "          \"from (select *, row_number() OVER (PARTITION BY customer_id ORDER BY order_date desc) as rn from \"+\n",
    "          \"(select s.customer_id as customer_id, order_date, product_id, join_date from Sales s \"+ \n",
    "          \"inner join Members m on s.customer_id = m.customer_id where order_date < join_date) temp) tf \"+\n",
    "          \"where rn = 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 8:\n",
    "\n",
    "¿Cuál es el total de artículos y la cantidad gastada por cada miembro antes de convertirse en miembro?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+----------+\n",
      "|customer_id|count(order_date)|sum(price)|\n",
      "+-----------+-----------------+----------+\n",
      "|          B|                3|        40|\n",
      "|          A|                2|        25|\n",
      "+-----------+-----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.join(members, [\"customer_id\"],\"inner\")\\\n",
    "     .where(col(\"order_date\")<col(\"join_date\"))\\\n",
    "     .join(menu, [\"product_id\"],\"inner\")\\\n",
    "     .select(\"customer_id\",\"order_date\",\"product_id\",\"price\")\\\n",
    "     .groupBy(\"customer_id\").agg({\"order_date\":\"count\",\"price\":\"sum\"})\\\n",
    "     .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----+\n",
      "|customer_id|countSales|Total|\n",
      "+-----------+----------+-----+\n",
      "|          B|         3|   40|\n",
      "|          A|         2|   25|\n",
      "+-----------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.createOrReplaceTempView(\"Sales\")\n",
    "members.createOrReplaceTempView(\"Members\")\n",
    "menu.createOrReplaceTempView(\"Menu\")\n",
    "\n",
    "spark.sql(\"select customer_id, count(order_date) as countSales, sum(price) as Total from \"+\n",
    "          \"(select s.customer_id, order_date, s.product_id, price from Sales s \"+\n",
    "          \"inner join Members m on s.customer_id = m.customer_id \"+\n",
    "          \"inner join Menu me on s.product_id = me.product_id \"+\n",
    "          \"where order_date < join_date) temp group by customer_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 9:\n",
    "\n",
    "Si cada $ 1 gastado equivale a 10 puntos y el sushi tiene un multiplicador de puntos 2x, ¿cuántos puntos tendría cada cliente?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|customer_id|sum(points)|\n",
      "+-----------+-----------+\n",
      "|          B|        940|\n",
      "|          C|        360|\n",
      "|          A|        860|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.join(menu,[\"product_id\"]).withColumn(\"mult\",col(\"product_name\")==\"sushi\")\\\n",
    "     .withColumn(\"mult\",col(\"mult\").cast(\"int\"))\\\n",
    "     .withColumn(\"points\",col(\"Price\")*10 + col(\"mult\")*(col(\"Price\")*10))\\\n",
    "     .groupBy(\"customer_id\").sum(\"points\")\\\n",
    "     .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|customer_id|total_points|\n",
      "+-----------+------------+\n",
      "|          B|         940|\n",
      "|          C|         360|\n",
      "|          A|         860|\n",
      "+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.createOrReplaceTempView(\"Sales\")\n",
    "menu.createOrReplaceTempView(\"Menu\")\n",
    "\n",
    "spark.sql(\"select customer_id, sum(points) as total_points from \"+\n",
    "          \"(select customer_id,price*10 + mult*price*10 as points from \"+\n",
    "          \"(select *,case when m.product_name = 'sushi' then 1 else 0 end as mult from \"+\n",
    "          \"Sales s inner join Menu m on s.product_id = m.product_id) temp) tf \"+\n",
    "          \"group by customer_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 10\n",
    "\n",
    "En la primera semana después de que un cliente se une al programa (incluida la fecha\n",
    "de ingreso), gana el doble de puntos en todos los artículos, no solo en sushi. ¿Cuántos\n",
    "puntos tienen los clientes A y B a fines de enero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|customer_id|sum(points)|\n",
      "+-----------+-----------+\n",
      "|          B|        940|\n",
      "|          A|       1370|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "sales.join(menu,[\"product_id\"],\"inner\")\\\n",
    "     .join(members,[\"customer_id\"],\"inner\")\\\n",
    "     .withColumn(\"mult_member\",when(col(\"order_date\")>=col(\"join_date\"),1).when(col(\"product_name\")==\"sushi\",1).otherwise(0))\\\n",
    "     .withColumn(\"mult_member\",col(\"mult_member\").cast(\"int\"))\\\n",
    "     .withColumn(\"points\",col(\"Price\")*10 + col(\"mult_member\")*(col(\"Price\")*10))\\\n",
    "     .filter(col(\"order_date\")<'2021-01-31')\\\n",
    "     .groupBy(\"customer_id\").sum(\"points\")\\\n",
    "     .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|customer_id|total_points|\n",
      "+-----------+------------+\n",
      "|          B|         940|\n",
      "|          A|        1370|\n",
      "+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.createOrReplaceTempView(\"Sales\")\n",
    "members.createOrReplaceTempView(\"Members\")\n",
    "menu.createOrReplaceTempView(\"Menu\")\n",
    "\n",
    "spark.sql(\"select customer_id, sum(points) as total_points from \"+\n",
    "          \"(select customer_id,order_date, price*10 + mult_member*price*10 as points from \"+\n",
    "          \"(select *, case when order_date>=join_date then 1 when product_name='sushi' then 1 else 0 end as mult_member from \"+\n",
    "          \"(select s.customer_id,s.order_date,m.product_name,m.price,me.join_date from Sales s \"+\n",
    "          \"inner join Menu m on s.product_id = m.product_id \"+\n",
    "          \"inner join Members me on s.customer_id = me.customer_id) temp) temp2 \"+\n",
    "          \"where order_date<'2021-01-31') tf group by customer_id\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('spark_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d2d0e2e36de1b6991818fcd8d87c2a832a0ca30b43f9d71a76baed6f56e906dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
